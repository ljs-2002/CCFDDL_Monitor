import argparse
import json
import os
import re
import smtplib
import time
import urllib
from collections import Counter
from datetime import datetime, timedelta, timezone
from email.header import Header
from email.mime.text import MIMEText

import requests
import yaml
from dateutil import parser as date_parser
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_fixed

# --- é…ç½®ä¸ç¯å¢ƒè¯»å– ---
CCF_PATH = "ccf-repo/conference"
DATA_DIR = "data"
STATE_FILE = os.path.join(DATA_DIR, "state.json")
KB_FILE = os.path.join(DATA_DIR, "knowledge_base.json")
INTERESTED_AREAS = os.environ.get("INTERESTED_AREAS", "AI,NW,DB,SC").split(",")
MAX_PAPERS_PER_YEAR = int(os.environ.get("MAX_PAPERS_PER_YEAR", "250"))

# LLM é…ç½®
LLM_API_KEY = os.environ.get("LLM_API_KEY")
LLM_BASE_URL = os.environ.get("LLM_BASE_URL", "https://api.openai.com/v1")
LLM_MODEL = os.environ.get("LLM_MODEL", "gpt-3.5-turbo")
PUSHPLUS_TOKEN = os.environ.get("PUSHPLUS_TOKEN")

client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)
CCF_SUB_MAP = {
    "AI": "äººå·¥æ™ºèƒ½ (AI)",
    "NW": "è®¡ç®—æœºç½‘ç»œ (NW)",
    "SE": "è½¯ä»¶å·¥ç¨‹/ç³»ç»Ÿè½¯ä»¶/ç¨‹åºè®¾è®¡è¯­è¨€ (SE)",
    "DB": "æ•°æ®åº“/æ•°æ®æŒ–æ˜/å†…å®¹æ£€ç´¢ (DB)",
    "CT": "è®¡ç®—æœºç§‘å­¦ç†è®º (CT)",
    "SC": "ç½‘ç»œä¸ä¿¡æ¯å®‰å…¨ (SC)",
    "CG": "è®¡ç®—æœºå›¾å½¢å­¦ä¸å¤šåª’ä½“ (CG)",
    "HI": "äººæœºäº¤äº’/æ™®é€‚è®¡ç®— (HI)",
    "MX": "äº¤å‰/ç»¼åˆ/æ–°å…´ (MX)",
    "DS": "è®¡ç®—æœºä½“ç³»ç»“æ„/å¹¶è¡Œä¸åˆ†å¸ƒè®¡ç®—/å­˜å‚¨ç³»ç»Ÿ (DS)",  # æœ‰äº›æ—§æ•°æ®å¯èƒ½æœ‰
}


# --- æ¨¡å— 1: æ—¶åŒºå¤„ç† ---
def get_timezone_offset(tz_str):
    tz = tz_str.strip().upper() if tz_str else "UTC"
    alias_map = {"AOE": -12, "EST": -5, "EDT": -4, "CST": 8, "JST": 9}
    if tz in alias_map:
        return alias_map[tz]
    try:
        if tz.startswith("UTC"):
            part = tz.replace("UTC", "")
            if not part:
                return 0
            return int(part)
    except:
        pass
    return 0


def convert_to_cst(date_str, timezone_str):
    """
    å°†ä¼šè®®å½“åœ°æ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8)
    """
    if not date_str or date_str == "TBD":
        return "TBD"

    try:
        dt = date_parser.parse(date_str)
        offset = get_timezone_offset(timezone_str)
        # åŸç†ï¼šåŸå§‹æ—¶é—´ - åŸæ—¶åŒºåç§» + 8 (CSTåç§»)
        # ä¾‹å¦‚ AoE (-12) 1æ—¥ 23:59 -> UTC 2æ—¥ 11:59 -> CST 2æ—¥ 19:59
        cst_time = dt - timedelta(hours=offset) + timedelta(hours=8)
        return cst_time.strftime("%Y-%m-%d %H:%M:%S (CST)")
    except Exception as e:
        print(
            f"   [Error] Date parse error: {e} for date_str: {date_str} and timezone_str: {timezone_str}"
        )
        return f"{date_str} (Parse Error)"


# --- æ¨¡å— 2: DBLP æ•°æ®è·å– ---
def fetch_dblp_papers(dblp_venue_key, year, limit=1000):
    """
    è·å–è®ºæ–‡æ ‡é¢˜å’Œé“¾æ¥
    dblp_venue_key: ä¾‹å¦‚ 'iwqos', 'iccv'
    year: ä¾‹å¦‚ 2022
    """
    print(
        f"   [DBLP] Fetching venue:'{dblp_venue_key}' year:'{year}' (Limit: {limit})..."
    )
    papers = []

    try:
        # ä¿®æ­£ï¼šä½¿ç”¨æ ‡å‡†çš„ query string è¯­æ³•: "venue:<name> year:<year>"
        # å¹¶è¿›è¡Œ URL ç¼–ç 
        q_str = f"venue:{dblp_venue_key} year:{year}"
        query_encoded = urllib.parse.quote(q_str)

        url = (
            f"https://dblp.org/search/publ/api?q={query_encoded}&h={limit}&format=json"
        )

        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        data = resp.json()
        print(f"   [DBLP] {data.get('result', {}).get('status', '')}")

        hits = data.get("result", {}).get("hits", {}).get("hit", [])

        if not hits:
            print(f"   [DBLP] No hits found for {q_str}")

        for hit in hits:
            info = hit.get("info", {})
            title = info.get("title", "No Title")

            # æå–é“¾æ¥ï¼šä¼˜å…ˆå– DOI (ee), å…¶æ¬¡å– DBLP é¡µé¢ (url)
            link = ""
            ee = info.get("ee")
            if isinstance(ee, list):
                link = ee[0]
            elif isinstance(ee, str):
                link = ee
            else:
                link = info.get("url", "")

            papers.append({"title": title, "link": link})

        time.sleep(1.5)  # éµå®ˆ DBLP API ç¤¼ä»ª
    except Exception as e:
        print(f"   [Error] DBLP fetch failed: {e}")

    return papers


# --- æ¨¡å— 3: LLM åˆ†ææµç¨‹ ---
@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def llm_stage1_extract_tags(papers_batch):
    papers_text = "\n".join(
        [f"[{i + 1}] {p['title']}" for i, p in enumerate(papers_batch)]
    )

    # Framework: CO-STAR
    prompt = f"""
    # Context: Senior CS Taxonomist analyzing conference papers.
    # Objective: Extract 1-5 academic tags per paper.
    # Constraints: 
    - Focus: 'Problem/Task' and 'Context/Domain' (e.g., "Encrypted Traffic Classification", "BGP Security").
    - Exclude: Generic methods like GNN, CNN, RL, Transformer.
    - Style: Professional English.
    # Example:
    Title: "Graph-based Anomaly Detection in SDN" -> Tags: ["Software-Defined Networking", "Network Anomaly Detection"]
    
    # Input:
    {papers_text}
    # Response: Strict JSON array of strings.
    """

    response = client.chat.completions.create(
        model=LLM_MODEL, messages=[{"role": "user", "content": prompt}], temperature=0.1
    )
    content = response.choices[0].message.content.strip()
    usage = response.usage
    usage_stats = {
        "prompt_tokens": usage.prompt_tokens,
        "completion_tokens": usage.completion_tokens,
        "total_tokens": usage.total_tokens,
    }

    try:
        json_str = re.search(r"\[.*\]", content, re.DOTALL).group()
        tags = json.loads(json_str)
    except:
        tags = re.findall(r'"([^"]+)"', content)

    return [str(t).strip() for t in tags], usage_stats


@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))
def llm_stage2_summarize(tag_counts, conference_name, year, total_papers):
    stats_text = json.dumps(tag_counts, indent=2)

    # Framework: CO-STAR
    prompt = f"""
    # Context: TPC Chair of {conference_name} ({year}).
    # Objective: Cluster tags into 5-10 high-level "Research Themes".
    # Guidelines:
    1. Taxonomy: Use broad categories (e.g., "Network Infrastructure & Protocol Security" instead of "NIDS").
    2. Format: Return a JSON list of objects: {{"name": "Chinese(English)", "ratio": "X%", "description": "..."}}.
    3. Mandatory Single Field: The "description" MUST include both the intro and sub-tags in this format: 
       "æœ¬ä¸»é¢˜ç ”ç©¶[ç®€çŸ­ä»‹ç»]ã€‚æ¶µç›–ï¼š[ç»†åˆ†æ–¹å‘1]ã€[ç»†åˆ†æ–¹å‘2]ç­‰ã€‚"
    # Example:
    {{
      "name": "å¯ä¿¡è®¡ç®—ä¸ç³»ç»Ÿå®‰å…¨ (Trustworthy Computing & System Security)",
      "ratio": "15%",
      "description": "æ¢è®¨æ„å»ºè½¯ç¡¬ä»¶ä¸€ä½“åŒ–çš„å®‰å…¨è¿è¡Œç¯å¢ƒã€‚æ¶µç›–ï¼šæœºå¯†è®¡ç®—ã€ä¾§ä¿¡é“åˆ†æã€å›ºä»¶å®‰å…¨ç­‰ã€‚"
    }}

    # Data: {stats_text}
    # Total Sample Size: {total_papers}
    # Response: JSON only.
    """

    response = client.chat.completions.create(
        model=LLM_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    content = response.choices[0].message.content.strip()
    usage = response.usage
    usage_stats = {
        "prompt_tokens": usage.prompt_tokens,
        "completion_tokens": usage.completion_tokens,
        "total_tokens": usage.total_tokens,
    }

    try:
        json_str = re.search(r"\[.*\]", content, re.DOTALL).group()
        return json.loads(json_str), usage_stats
    except:
        return [], usage_stats


def analyze_year_data(
    dblp_name, year, conf_display_name, max_papers=100, verbose=False
):
    papers = fetch_dblp_papers(dblp_name, year, limit=max_papers)
    if not papers:
        print(f"   [Analysis] No papers found for {year}.")
        return None

    # å…¨å±€è®¡æ•°å™¨ï¼šè®°å½•æ¯ä¸ª Tag åœ¨å¤šå°‘ç¯‡è®ºæ–‡ä¸­å‡ºç°è¿‡
    global_tag_counter = Counter()
    total_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    batch_size = 10

    print(f"   [Analysis] Processing {len(papers)} papers via LLM ({LLM_MODEL})...")

    # --- Stage 1: Batch Processing ---
    for i in range(0, len(papers), batch_size):
        batch = papers[i : i + batch_size]

        if verbose:
            print(f"\n   [Debug] Batch {i // batch_size + 1} Inputs:")
            for p in batch:
                print(f"      - {p['title']}")

        try:
            # è·å–æœ¬æ‰¹æ¬¡çš„æ‰€æœ‰ Tags
            batch_tags_flat, usage = llm_stage1_extract_tags(batch)

            # ç´¯åŠ ç”¨é‡
            for k in total_usage:
                total_usage[k] += usage.get(k, 0)

            for tag in batch_tags_flat:
                # ç®€å•æ¸…æ´—ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ï¼Œè½¬ Title Case æ–¹ä¾¿ç»Ÿè®¡
                clean_tag = tag.strip()
                if clean_tag:
                    global_tag_counter[clean_tag] += 1

            if verbose:
                print(f"   [Debug] Batch Tags: {batch_tags_flat}")
                print(f"   [Debug] Batch Usage: {usage['total_tokens']}")

        except Exception as e:
            print(f"     Batch {i // batch_size + 1} failed: {e}")

    if not global_tag_counter:
        return None

    # --- Stage 2: Summarization ---
    top_tags_map = dict(global_tag_counter)

    print(f"   [Analysis] Summarizing from {len(top_tags_map)} distinct tags...")

    # ä¼ å…¥ total_papers (len(papers)) ä»¥ä¾¿è®¡ç®—æ­£ç¡®æ¯”ä¾‹
    final_summary, usage_s2 = llm_stage2_summarize(
        tag_counts=top_tags_map,
        conference_name=conf_display_name,
        year=year,
        total_papers=len(papers),
    )

    for k in total_usage:
        total_usage[k] += usage_s2.get(k, 0)

    if verbose:
        print(
            f"\n   [Debug] Final Summary:\n{json.dumps(final_summary, indent=2, ensure_ascii=False)}"
        )
        print(f"   [Debug] Total Cost: {total_usage}")

    return {
        "titles_count": len(papers),
        "summary": final_summary,
        "token_usage": total_usage,
        "updated_at": datetime.now().strftime("%Y-%m-%d"),
    }


# --- æ¨¡å— 4: æ•°æ®å­˜å‚¨ä¸é€šçŸ¥ ---
def get_notification_body(info, kb_record):
    """
    ç»„è£… Markdown é€šçŸ¥å†…å®¹ï¼ŒæŒ‰å¹´ä»½å€’åºæ’åˆ—ï¼ŒTag æŒ‰æ¯”ä¾‹æ’åºã€‚
    """
    rank = info["rank"]
    rank_color_map = {"A": "#FF0000", "B": "#FFA500", "C": "#008000", "N": "#808080"}
    color = rank_color_map.get(rank, "#000000")
    rank_html = f'<font color="{color}">CCF-{rank}</font>'

    # 1. è·å–è¿‘ 3 å¹´æ•°æ®å¹¶æŒ‰å¹´ä»½å€’åº
    years = sorted(kb_record.keys(), reverse=True)[:3] if kb_record else []

    analysis_section = ""
    total_tokens_consumed = 0

    if not years:
        analysis_section = "âš ï¸ **æš‚æ— å†å²è®ºæ–‡è¶‹åŠ¿åˆ†æ**"
    else:
        analysis_section = "ğŸ§  **è¿‘ 3 å¹´å­¦æœ¯è¶‹åŠ¿åˆ†æ**\n"
        for y in years:
            data = kb_record[y]
            summary = data.get("summary", [])
            total_tokens_consumed += data.get("token_usage", {}).get("total_tokens", 0)

            if not summary:
                continue

            # 2. æŒ‰æ¯”ä¾‹(ratio)å¯¹å½“å‰å¹´ä»½çš„ Tag è¿›è¡Œé™åºæ’åˆ—
            sorted_summary = sorted(
                summary,
                key=lambda x: float(x.get("ratio", "0%").strip("%")),
                reverse=True,
            )

            analysis_section += (
                f"\n#### ğŸ“… {y} å¹´ (æ ·æœ¬é‡: {data.get('titles_count', '?')} ç¯‡)\n"
            )
            for tag in sorted_summary:
                name = tag.get("name", "Unknown")
                desc = tag.get("description", "")
                ratio = tag.get("ratio", "")
                analysis_section += f"- **{name}** `({ratio})`\n  - {desc}\n"

    token_footer = (
        f"\n---\n###### ğŸ’ LLM Token Cost: {total_tokens_consumed} (Analysis Session)"
        if total_tokens_consumed > 0
        else ""
    )

    msg = f"""
## ğŸ“¢ {info["title"]} {info["year"]} æ›´æ–°æé†’
> {info["description"]}

- **é¢†åŸŸ**: {info["sub"]} | **ç­‰çº§**: {rank_html}
- **æ—¶é—´**: {info["date"]} | **åœ°ç‚¹**: {info["place"]}
- **å®˜ç½‘**: [ç‚¹å‡»è·³è½¬]({info["link"]})

---
### â° å…³é”®æˆªç¨¿ (åŒ—äº¬æ—¶é—´)
- **æ‘˜è¦æˆªæ­¢**: {info["abs_deadline"]}
- **å…¨æ–‡æˆªæ­¢**: {info["main_deadline"]}

---
{analysis_section}
{token_footer}
    """
    return msg


def send_pushplus(title, content):
    if not PUSHPLUS_TOKEN:
        print("   [Notify] Skip: No PUSHPLUS_TOKEN.")
        return

    url = "http://www.pushplus.plus/send"
    data = {
        "token": PUSHPLUS_TOKEN,
        "title": title,
        "content": content,
        "template": "markdown",
    }
    try:
        requests.post(url, json=data, timeout=5)
        print("   [Notify] PushPlus sent successfully.")
    except Exception as e:
        print(f"   [Notify] Failed: {e}")


def get_email_body(info, kb_record):
    """
    ç»„è£…çº¯æ–‡æœ¬é‚®ä»¶å†…å®¹ï¼ˆæ—  Markdown ç¬¦å·ï¼‰
    """
    years = sorted(kb_record.keys(), reverse=True)[:3] if kb_record else []

    analysis_text = ""
    if not years:
        analysis_text = "æš‚æ— å†å²è®ºæ–‡è¶‹åŠ¿åˆ†ææ•°æ®ã€‚"
    else:
        for y in years:
            data = kb_record[y]
            analysis_text += (
                f"\nã€{y} å¹´è¶‹åŠ¿ (æ ·æœ¬é‡: {data.get('titles_count', '?')} ç¯‡)ã€‘\n"
            )
            summary = data.get("summary", [])
            # æŒ‰æ¯”ä¾‹æ’åº
            sorted_summary = sorted(
                summary,
                key=lambda x: float(x.get("ratio", "0%").strip("%")),
                reverse=True,
            )
            for tag in sorted_summary:
                analysis_text += f"- {tag.get('name')} (æ¯”ä¾‹: {tag.get('ratio')})\n"
                analysis_text += f"  è¯¦æƒ…: {tag.get('description')}\n"

    body = f"""
ä¼šè®®æ›´æ–°æé†’ï¼š{info["title"]} {info["year"]}

--------------------------------------------------

ä¼šè®®æè¿°ï¼š{info["description"]}

[åŸºæœ¬ä¿¡æ¯]
- é¢†åŸŸï¼š{info["sub"]}
- ç­‰çº§ï¼šCCF-{info["rank"]}
- æ—¶é—´ï¼š{info["date"]}
- åœ°ç‚¹ï¼š{info["place"]}
- å®˜ç½‘ï¼š{info["link"]}

[é‡è¦æˆªç¨¿æ—¶é—´ (åŒ—äº¬æ—¶é—´)]
- æ‘˜è¦æˆªæ­¢ï¼š{info["abs_deadline"]}
- å…¨æ–‡æˆªæ­¢ï¼š{info["main_deadline"]}

[è¿‘3å¹´å­¦æœ¯è¶‹åŠ¿æ·±åº¦åˆ†æ]
{analysis_text}
--------------------------------------------------
æç¤ºï¼šæœ¬é‚®ä»¶ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œå†å²åˆ†æåŸºäº DBLP æ•°æ®ã€‚
"""
    return body


def send_email(title, content):
    """
    é€šè¿‡ SMTP å‘é€é‚®ä»¶é€šçŸ¥
    """
    # ç¯å¢ƒå˜é‡è¯»å–
    smtp_host = os.environ.get("SMTP_HOST")
    smtp_port = int(os.environ.get("SMTP_PORT", "465"))
    smtp_user = os.environ.get("SMTP_USER")
    smtp_pass = os.environ.get("SMTP_PASS")
    receiver = os.environ.get("RECEIVER_EMAIL")

    # åªæœ‰é…ç½®äº†å®Œæ•´ä¿¡æ¯æ‰ä¼šå‘é€
    if not all([smtp_host, smtp_user, smtp_pass, receiver]):
        print("   [Notify] Email skip: Configuration incomplete.")
        return

    message = MIMEText(content, "plain", "utf-8")
    message["From"] = smtp_user
    message["To"] = receiver
    message["Subject"] = Header(title, "utf-8")

    try:
        smtp_obj = smtplib.SMTP_SSL(smtp_host, smtp_port)
        smtp_obj.login(smtp_user, smtp_pass)
        smtp_obj.sendmail(smtp_user, [receiver], message.as_string())
        smtp_obj.quit()
        print("   [Notify] Email sent successfully.")
    except Exception as e:
        print(f"   [Notify] Email failed: {e}")


# --- æ¨¡å— 5: æ ¸å¿ƒæµç¨‹æ§åˆ¶ ---


def get_timeline_status(timeline_list, timezone_str):
    """
    ä» timeline åˆ—è¡¨ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¿˜æ²¡è¿‡æœŸçš„ deadlineã€‚
    å¦‚æœéƒ½è¿‡æœŸäº†ï¼Œè¿”å›æœ€åä¸€ä¸ªã€‚
    è¿”å›: (selected_timeline_item, status_text)
    """
    if not timeline_list:
        return {}, "æœªå®š"

    now_utc = datetime.now(timezone.utc)
    tz_offset = get_timezone_offset(timezone_str)

    # å¯»æ‰¾ç¬¬ä¸€ä¸ªè¿˜æ²¡è¿‡çš„æ—¶é—´
    for item in timeline_list:
        deadline_str = item.get("deadline", "TBD")
        if deadline_str == "TBD":
            continue

        try:
            # è§£æ deadline
            dl_dt = date_parser.parse(deadline_str)
            # è½¬æ¢ä¸º UTC æ—¶é—´ä»¥ä¾¿æ¯”è¾ƒ: (Local Time - Offset = UTC)
            dl_utc = dl_dt - timedelta(hours=tz_offset)
            # å°†å…¶è½¬æ¢ä¸ºæ—¶åŒºæ„ŸçŸ¥çš„ UTC å¯¹è±¡ï¼Œä»¥ä¾¿ä¸ now_utc æ¯”è¾ƒ
            dl_utc = dl_utc.replace(tzinfo=timezone.utc)

            if dl_utc > now_utc:
                # æ‰¾åˆ°æœªæ¥çš„æ—¶é—´
                return item, "è¿›è¡Œä¸­"
        except Exception as e:
            print(
                f"   [Error] Timeline date parse error: {e} for deadline_str: {deadline_str}"
            )
            continue

    # å¦‚æœéƒ½è¿‡æœŸäº†ï¼Œè¿”å›æœ€åä¸€ä¸ª
    return timeline_list[-1], "å·²æˆªæ­¢"


def process_updates(local_test_file=None):
    """
    æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼šæ”¯æŒåˆå§‹åŒ–éƒ¨ç½²ã€é¢†åŸŸè¿‡æ»¤ã€åŸºäºæœ€æ–°å¹´ä»½çš„å†å²åˆ†æã€‚
    """
    # 1. åŠ è½½æŒä¹…åŒ–æ•°æ®
    state, kb = {}, {}
    if os.path.exists(STATE_FILE):
        with open(STATE_FILE, "r") as f:
            state = json.load(f)
    if os.path.exists(KB_FILE):
        with open(KB_FILE, "r") as f:
            kb = json.load(f)

    is_initial_run = not bool(state)
    changes_detected = False
    files_to_process = [local_test_file] if local_test_file else []

    if not local_test_file:
        for root, _, files in os.walk(CCF_PATH):
            for f in files:
                if f.endswith(".yml"):
                    files_to_process.append(os.path.join(root, f))

    for file_path in files_to_process:
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                raw_data = yaml.safe_load(f)
                data_list = raw_data if isinstance(raw_data, list) else [raw_data]
            except Exception as e:
                print(f"   [Error] Load failed {file_path}: {e}")
                continue

            for data in data_list:
                if not data:
                    continue

                # é¢†åŸŸè¿‡æ»¤
                sub_code = data.get("sub", "")
                if "ALL" not in INTERESTED_AREAS and sub_code not in INTERESTED_AREAS:
                    continue

                dblp_name = data.get("dblp") or data.get("title", "").lower()

                # æ ¸å¿ƒä¿®æ”¹ï¼šæ‰¾å‡ºè¯¥ä¼šè®®è®°å½•ä¸­çš„æœ€æ–°å¹´ä»½
                all_confs = data.get("confs", [])
                if not all_confs:
                    continue
                max_year_in_data = max(c.get("year", 0) for c in all_confs)

                for conf in all_confs:
                    conf_id = str(conf.get("id"))
                    current_conf_year = conf.get("year")
                    tl_data = conf.get("timeline", [{}])[0]
                    fingerprint = {"year": current_conf_year, "timeline": tl_data}

                    # åˆ¤æ–­æ›´æ–°ï¼šæ–°ä¼šè®®ã€æŒ‡çº¹å˜åŠ¨ã€æˆ–æµ‹è¯•æ¨¡å¼
                    old_fp = state.get(conf_id)
                    is_new_update = old_fp != fingerprint

                    # åªæœ‰å‘ç”Ÿæ›´æ–°æˆ–è€…æ˜¯åˆå§‹åŒ–/æµ‹è¯•æ¨¡å¼æ—¶ï¼Œæ‰æ‰§è¡Œæ¨é€æµç¨‹
                    if is_new_update or local_test_file or is_initial_run:
                        state[conf_id] = fingerprint
                        changes_detected = True

                        # åªé’ˆå¯¹â€œæœ€æ–°å¹´ä»½â€çš„æ¡ç›®æ‰§è¡Œå†å²æ·±åº¦åˆ†æï¼Œé¿å…æ—§æ¡ç›®è§¦å‘é‡å¤åˆ†æ
                        if current_conf_year == max_year_in_data:
                            print(
                                f"ğŸš€ Processing Latest: {conf_id} ({current_conf_year})"
                            )

                            # è·å–å¹¶è½¬æ¢æˆªç¨¿æ—¥æœŸ
                            target_tl, status = get_timeline_status(
                                conf.get("timeline", []), conf.get("timezone")
                            )
                            info = {
                                "title": data.get("title"),
                                "description": data.get("description"),
                                "sub": CCF_SUB_MAP.get(sub_code, sub_code),
                                "rank": data.get("rank", {}).get("ccf"),
                                "year": current_conf_year,
                                "date": conf.get("date"),
                                "place": conf.get("place"),
                                "link": conf.get("link"),
                                "abs_deadline": convert_to_cst(
                                    target_tl.get("abstract_deadline"),
                                    conf.get("timezone"),
                                ),
                                "main_deadline": convert_to_cst(
                                    target_tl.get("deadline"), conf.get("timezone")
                                )
                                + (" (å·²è¿‡)" if status == "å·²æˆªæ­¢" else ""),
                            }

                            # å†å²æ•°æ®è·å–ï¼šä»å½“å‰æœ€æ–°å¹´ä»½å¾€å‰æ¨ 3 å¹´ (e.g. 2025 -> 2024, 2023, 2022)
                            if dblp_name not in kb:
                                kb[dblp_name] = {}
                            target_years = [current_conf_year - i for i in range(1, 4)]

                            for y in target_years:
                                str_y = str(y)
                                if str_y not in kb[dblp_name]:
                                    print(
                                        f"   [DBLP Analysis] Fetching {dblp_name} for year {y}..."
                                    )
                                    res = analyze_year_data(
                                        dblp_name,
                                        y,
                                        info["title"],
                                        max_papers=MAX_PAPERS_PER_YEAR,
                                    )
                                    if res:
                                        kb[dblp_name][str_y] = res

                            # æ¨é€é€šçŸ¥ (åˆå§‹åŒ–æ¨¡å¼ä¸æ¨é€ï¼Œé˜²æ­¢çˆ†è¡¨)
                            if not is_initial_run:
                                msg_body = get_notification_body(
                                    info, kb.get(dblp_name)
                                )
                                send_pushplus(f"{info['title']} æ›´æ–°æé†’", msg_body)
                                mail_body = get_email_body(info, kb.get(dblp_name))
                                send_email(f"{info['title']} æ›´æ–°æé†’", mail_body)
                        else:
                            # å¦‚æœä¸æ˜¯æœ€æ–°å¹´ä»½ï¼Œä»…æ›´æ–°çŠ¶æ€æŒ‡çº¹ï¼Œä¸è§¦å‘æ·±åº¦åˆ†æå’Œæ¨é€
                            continue

    # æ— è®ºæ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œåªè¦æœ‰å˜åŠ¨å°±ä¿å­˜ï¼Œç¡®ä¿çŸ¥è¯†åº“ä¸æ–­ç´¯ç§¯
    if changes_detected:
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(STATE_FILE, "w") as f:
            json.dump(state, f, indent=2)
        with open(KB_FILE, "w") as f:
            json.dump(kb, f, indent=2)
        print(f"âœ… Data saved to {STATE_FILE} and {KB_FILE}")


# --- æœ¬åœ°æµ‹è¯•å…¥å£ ---
def run_local_test(yml_path):
    print(f"ğŸ”§ Starting LOCAL TEST with file: {yml_path}")
    print("Ensure environment variables LLM_API_KEY and PUSHPLUS_TOKEN are set.")

    if not os.path.exists(yml_path):
        print("File not found!")
        return

    process_updates(local_test_file=yml_path)


if __name__ == "__main__":
    args_parser = argparse.ArgumentParser()
    args_parser.add_argument("--test", help="Path to a single yml file to test")
    args = args_parser.parse_args()

    if args.test:
        # run_local_test å‡½æ•°é€»è¾‘ä¸å˜ï¼Œåªéœ€ç¡®ä¿å®ƒè°ƒç”¨ process_updates
        print(f"ğŸ”§ Starting LOCAL TEST with file: {args.test}")
        if not os.path.exists(args.test):
            print(f"âŒ File not found: {args.test}")
        else:
            process_updates(local_test_file=args.test)
    else:
        process_updates()
